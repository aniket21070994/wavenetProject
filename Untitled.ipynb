{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1RsVWfwmOslnhB2z2jTUL777i7sV2Uo6H",
      "authorship_tag": "ABX9TyMXygL0+MS4zfEjWFTvH3+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniket21070994/wavenetProject/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRt3nJyQIldi",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# prompt: code to unzip this file /content/archive.zip\n",
        "\n",
        "!unzip /content/c+.zip -d /content/ck\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip  install mediapipe"
      ],
      "metadata": {
        "id": "Ha6SLKa4FvQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mearg dataset"
      ],
      "metadata": {
        "id": "6tAey2WT9rj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load MediaPipe face detection\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.6)\n",
        "\n",
        "def align_face(image):\n",
        "    \"\"\"Align face using MediaPipe face detection.\"\"\"\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = face_detection.process(image_rgb)\n",
        "\n",
        "    if results.detections:\n",
        "        for detection in results.detections:\n",
        "            bbox = detection.location_data.relative_bounding_box\n",
        "            h_img, w_img, _ = image.shape\n",
        "            x_min = int(bbox.xmin * w_img)\n",
        "            y_min = int(bbox.ymin * h_img)\n",
        "            width = int(bbox.width * w_img)\n",
        "            height = int(bbox.height * h_img)\n",
        "\n",
        "            # Ensure valid face bounding box\n",
        "            if width > 0 and height > 0 and x_min >= 0 and y_min >= 0 and (x_min + width) <= w_img and (y_min + height) <= h_img:\n",
        "                face = image[y_min:y_min+height, x_min:x_min+width]\n",
        "                if face.size != 0:\n",
        "                    return cv2.resize(face, (64, 64))\n",
        "\n",
        "    # Default fallback: Resize original image if no face detected\n",
        "    return cv2.resize(image, (64, 64))\n",
        "\n",
        "def apply_histogram_equalization(image):\n",
        "    \"\"\"Apply histogram equalization on the Y channel.\"\"\"\n",
        "    img_yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
        "    img_yuv[:, :, 0] = cv2.equalizeHist(img_yuv[:, :, 0])\n",
        "    return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
        "\n",
        "# Augmentation pipeline (Fixed `CoarseDropout` issue)\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.CoarseDropout(max_holes=3, max_size=20, p=0.5)  # Fixed CoarseDropout parameters\n",
        "])\n",
        "\n",
        "def process_fer2013(csv_path):\n",
        "    \"\"\"Load and preprocess FER2013 dataset from CSV.\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    images, labels = [], []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing FER2013\"):\n",
        "        pixels = np.fromstring(row['pixels'], sep=' ', dtype=np.uint8).reshape(48, 48)\n",
        "        image = cv2.cvtColor(pixels, cv2.COLOR_GRAY2BGR)  # Convert grayscale to BGR\n",
        "        image = align_face(image)\n",
        "        image = apply_histogram_equalization(image)\n",
        "        images.append(image)\n",
        "        labels.append(row['emotion'])\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "import re  # Import regex module\n",
        "\n",
        "def is_valid_hex(s):\n",
        "    \"\"\"Check if the string contains only valid hexadecimal characters.\"\"\"\n",
        "    return bool(re.fullmatch(r'[0-9a-fA-F]+', s))\n",
        "\n",
        "def process_ck_plus(csv_path):\n",
        "    \"\"\"Load and preprocess CK+ dataset from CSV (HEX-encoded images).\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if 'pixels' not in df.columns or 'emotion' not in df.columns:\n",
        "        print(\"Error: Missing 'pixels' or 'emotion' column in CK+ dataset.\")\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    images, labels = [], []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing CK+\"):\n",
        "        try:\n",
        "            pixel_data = str(row['pixels']).strip()\n",
        "\n",
        "            # Validate hexadecimal string before processing\n",
        "            if not is_valid_hex(pixel_data):\n",
        "                print(f\"Skipping invalid hex data at index {_}: {pixel_data[:20]}...\")  # Show first 20 chars\n",
        "                continue  # Skip this row\n",
        "\n",
        "            img_data = np.frombuffer(bytearray.fromhex(pixel_data), dtype=np.uint8)\n",
        "            image = cv2.imdecode(img_data, cv2.IMREAD_COLOR)\n",
        "\n",
        "            if image is None:\n",
        "                continue  # Skip corrupted images\n",
        "\n",
        "            image = align_face(image)\n",
        "            image = apply_histogram_equalization(image)\n",
        "            images.append(image)\n",
        "            labels.append(row['emotion'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Skipping invalid image at index {_}. Error: {e}\")\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def augment_dataset(images, labels, augment_factor=0.5):\n",
        "    \"\"\"Apply augmentation to a subset of the dataset.\"\"\"\n",
        "    num_augment = int(len(images) * augment_factor)\n",
        "    aug_images, aug_labels = [], []\n",
        "\n",
        "    for i in tqdm(range(num_augment), desc=\"Applying Augmentations\"):\n",
        "        img = images[i]\n",
        "        aug_img = transform(image=img)['image']\n",
        "        aug_images.append(aug_img)\n",
        "        aug_labels.append(labels[i])\n",
        "\n",
        "    return np.array(aug_images), np.array(aug_labels)\n",
        "\n",
        "# Load datasets\n",
        "fer_images, fer_labels = process_fer2013('/content/FER/fer2013.csv')\n",
        "ck_images, ck_labels = process_ck_plus('/content/ck/ckextended.csv')\n",
        "\n",
        "# Ensure CK+ dataset is not empty\n",
        "if ck_images.size == 0 or ck_labels.size == 0:\n",
        "    print(\"Warning: CK+ dataset is empty. Proceeding with FER2013 only.\")\n",
        "\n",
        "# Merge datasets safely\n",
        "if ck_images.size > 0 and ck_labels.size > 0:\n",
        "    all_images = np.concatenate((fer_images, ck_images), axis=0)\n",
        "    all_labels = np.concatenate((fer_labels, ck_labels), axis=0)\n",
        "else:\n",
        "    all_images = fer_images\n",
        "    all_labels = fer_labels\n",
        "\n",
        "# Apply augmentation to a subset\n",
        "aug_images, aug_labels = augment_dataset(all_images, all_labels, augment_factor=0.5)\n",
        "\n",
        "# Final dataset\n",
        "final_images = np.concatenate((all_images, aug_images), axis=0)\n",
        "final_labels = np.concatenate((all_labels, aug_labels), axis=0)\n",
        "\n",
        "# Save dataset in compressed format\n",
        "np.savez_compressed('final_dataset.npz', images=final_images, labels=final_labels)\n",
        "\n",
        "print(f\"âœ… Final dataset saved with {final_images.shape[0]} samples! ðŸš€\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n4uFjDPeaESt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train model\n"
      ],
      "metadata": {
        "id": "9uKMz7i-VKDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------------------\n",
        "# 1. Load & Preprocess the Merged Dataset\n",
        "# -------------------------------------------\n",
        "data = np.load('/content/final_dataset.npz', mmap_mode='r')\n",
        "images, labels = data['images'], data['labels']\n",
        "\n",
        "images = images.astype('float32') / 255.0\n",
        "num_classes = len(np.unique(labels))\n",
        "labels_onehot = tf.keras.utils.to_categorical(labels, num_classes)\n",
        "\n",
        "# Use dummy intensity values (0.5) since ground truth isn't provided\n",
        "intensity_targets = np.full((labels.shape[0], 1), 0.5, dtype='float32')\n",
        "\n",
        "# Get image dimensions (assumes images are stored as (batch, 64, 64, 3))\n",
        "print(\"Original image shape:\", images.shape)\n",
        "# DO NOT reshape images: we want to keep the shape (batch, 64, 64, 3)\n",
        "\n",
        "# Create a tf.data.Dataset pipeline for efficient training\n",
        "BATCH_SIZE = 16\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (images, {'emotion': labels_onehot, 'intensity': intensity_targets})\n",
        ")\n",
        "dataset = dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# -------------------------------------------\n",
        "# 2. Define the Simple FER Model without Attention\n",
        "# -------------------------------------------\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_simple_fer_model(input_shape=(64, 64, 3), num_classes=7):\n",
        "    inputs = Input(shape=input_shape, name='input_image')\n",
        "\n",
        "    # Convolutional layers for feature extraction\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)  # Output: 32x32x16\n",
        "\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)  # Output: 16x16x32\n",
        "\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)  # Output: 8x8x64\n",
        "\n",
        "    # Flatten and add dense layers for feature processing\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Dual output head:\n",
        "    # Emotion classification branch\n",
        "    emotion_output = Dense(num_classes, activation='softmax', name='emotion')(x)\n",
        "    # Intensity regression branch\n",
        "    intensity_output = Dense(1, activation='sigmoid', name='intensity')(x)\n",
        "\n",
        "    model = Model(inputs, [emotion_output, intensity_output], name=\"Simple_FER_Model_NoAttention\")\n",
        "    return model\n",
        "\n",
        "# -------------------------------------------\n",
        "# 3. Build, Compile, and Summarize the Model\n",
        "# -------------------------------------------\n",
        "num_classes = 7\n",
        "model = build_simple_fer_model(input_shape=(64, 64, 3), num_classes=num_classes)\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'emotion': 'categorical_crossentropy', 'intensity': 'mse'},\n",
        "              metrics={'emotion': 'accuracy', 'intensity': 'mae'})\n",
        "model.summary()\n",
        "\n",
        "# -------------------------------------------\n",
        "# 4. Train the Model\n",
        "# -------------------------------------------\n",
        "model.fit(dataset, epochs=20)\n"
      ],
      "metadata": {
        "id": "IRq0PWIcJS7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/FER.h5\");"
      ],
      "metadata": {
        "id": "ygis5y249DBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_simple_fer_model(input_shape=(64, 64, 3), num_classes=7):\n",
        "    inputs = Input(shape=input_shape, name='input_image')\n",
        "\n",
        "    # Convolutional layers for feature extraction\n",
        "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)  # Output: 32x32x16\n",
        "\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)  # Output: 16x16x32\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)  # Output: 8x8x64\n",
        "\n",
        "    # Flatten and add dense layers for feature processing\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Dual output head:\n",
        "    # Emotion classification branch\n",
        "    emotion_output = layers.Dense(num_classes, activation='softmax', name='emotion')(x)\n",
        "    # Intensity regression branch\n",
        "    intensity_output = layers.Dense(1, activation='sigmoid', name='intensity')(x)\n",
        "\n",
        "    # Create and return the model\n",
        "    model = Model(inputs, [emotion_output, intensity_output], name=\"Simple_FER_Model_NoAttention\")\n",
        "    return model\n",
        "\n",
        "# Build and compile the model\n",
        "num_classes = 7\n",
        "model = build_simple_fer_model(input_shape=(64, 64, 3), num_classes=num_classes)\n",
        "model.compile(optimizer='adam',\n",
        "              loss={'emotion': 'categorical_crossentropy', 'intensity': 'mse'},\n",
        "              metrics={'emotion': 'accuracy', 'intensity': 'mae'})\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "bAhVrPweRkkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping function"
      ],
      "metadata": {
        "id": "ueW2Jo8aG2mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def save_model(model, model_path):\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "    # Save the model\n",
        "    model.save(model_path)\n",
        "    print(f\"Model saved successfully at {model_path}\")\n",
        "\n",
        "# Example usage:\n",
        "# save_model(model, 'models/fer_model.h5')\n",
        "save_model(model, \"/content/project_FRM/fer_model\")\n"
      ],
      "metadata": {
        "id": "iYncKMtuPUE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def map_to_wavenet_condition(emotion_probs, intensity, num_classes=7, embedding_dim=16):\n",
        "    \"\"\"\n",
        "    Maps FER model outputs (emotion probabilities and intensity) into a 16-dimensional conditioning vector\n",
        "    for WaveNet.\n",
        "\n",
        "    Parameters:\n",
        "      emotion_probs: Tensor of shape (batch_size, num_classes) with emotion probabilities.\n",
        "      intensity: Tensor of shape (batch_size, 1) with intensity scores.\n",
        "      num_classes: Number of emotion classes (default is 7).\n",
        "      embedding_dim: Desired dimension for the conditioning vector (default is 16).\n",
        "\n",
        "    Returns:\n",
        "      A tensor of shape (batch_size, embedding_dim) that can be used as conditioning input for WaveNet.\n",
        "    \"\"\"\n",
        "    # Concatenate emotion probabilities and intensity along the last axis.\n",
        "    conditioning_input = tf.concat([emotion_probs, intensity], axis=-1)  # Shape: (batch_size, num_classes + 1)\n",
        "\n",
        "    # Pass the concatenated vector through two dense layers to produce the 16-dimensional embedding.\n",
        "    x = layers.Dense(32, activation='relu')(conditioning_input)\n",
        "    conditioning_vector = layers.Dense(embedding_dim, activation='relu')(x)\n",
        "\n",
        "    return conditioning_vector\n",
        "\n",
        "# --- Example Usage ---\n",
        "# Suppose your FER model outputs are as follows:\n",
        "# For demonstration, let's create dummy data:\n",
        "batch_size = 4\n",
        "num_classes = 7\n",
        "\n",
        "# Random emotion probabilities (ensure they sum to 1 via softmax)\n",
        "dummy_emotion_logits = tf.random.uniform((batch_size, num_classes))\n",
        "dummy_emotion_probs = tf.nn.softmax(dummy_emotion_logits, axis=-1)\n",
        "\n",
        "# Random intensity scores between 0 and 1\n",
        "dummy_intensity = tf.random.uniform((batch_size, 1), minval=0, maxval=1)\n",
        "\n",
        "# Map FER outputs to a 16-dimensional conditioning vector for WaveNet\n",
        "conditioning_vector = map_to_wavenet_condition(dummy_emotion_probs, dummy_intensity, num_classes, embedding_dim=16)\n",
        "print(\"Conditioning vector shape:\", conditioning_vector.shape)\n"
      ],
      "metadata": {
        "id": "6GwcdKxDG1y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WaveNet:"
      ],
      "metadata": {
        "id": "-KF8JXPdG9DN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jGEsLlUZHGDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}